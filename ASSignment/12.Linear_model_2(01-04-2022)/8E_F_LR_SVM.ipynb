{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HExLQrE4ZxR"
   },
   "source": [
    "<h1><font color='blue'> 8E and 8F: Finding the Probability P(Y==1|X)</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LuKrFzC4ZxV"
   },
   "source": [
    "<h2><font color='Geen'> 8E: Implementing Decision Function of SVM RBF Kernel</font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wES-wWN4ZxX"
   },
   "source": [
    "<font face=' Comic Sans MS' size=3>After we train a kernel SVM model, we will be getting support vectors and their corresponsing coefficients $\\alpha_{i}$\n",
    "\n",
    "Check the documentation for better understanding of these attributes: \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "<img src='https://i.imgur.com/K11msU4.png' width=500>\n",
    "\n",
    "As a part of this assignment you will be implementing the ```decision_function()``` of kernel SVM, here decision_function() means based on the value return by ```decision_function()``` model will classify the data point either as positive or negative\n",
    "\n",
    "Ex 1: In logistic regression After traning the models with the optimal weights $w$ we get, we will find the value $\\frac{1}{1+\\exp(-(wx+b))}$, if this value comes out to be < 0.5 we will mark it as negative class, else its positive class\n",
    "\n",
    "Ex 2: In Linear SVM After traning the models with the optimal weights $w$ we get, we will find the value of $sign(wx+b)$, if this value comes out to be -ve we will mark it as negative class, else its positive class.\n",
    "\n",
    "Similarly in Kernel SVM After traning the models with the coefficients $\\alpha_{i}$ we get, we will find the value of \n",
    "$sign(\\sum_{i=1}^{n}(y_{i}\\alpha_{i}K(x_{i},x_{q})) + intercept)$, here $K(x_{i},x_{q})$ is the RBF kernel. If this value comes out to be -ve we will mark $x_{q}$ as negative class, else its positive class.\n",
    "\n",
    "RBF kernel is defined as: $K(x_{i},x_{q})$ = $exp(-\\gamma ||x_{i} - x_{q}||^2)$\n",
    "\n",
    "For better understanding check this link: https://scikit-learn.org/stable/modules/svm.html#svm-mathematical-formulation\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z830CfMk4Zxa"
   },
   "source": [
    "## Task E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuBxHiCQ4Zxc"
   },
   "source": [
    "> 1. Split the data into $X_{train}$(60), $X_{cv}$(20), $X_{test}$(20)\n",
    "\n",
    "> 2. Train $SVC(gamma=0.001, C=100.)$ on the ($X_{train}$, $y_{train}$)\n",
    "\n",
    "> 3. Get the decision boundry values $f_{cv}$ on the $X_{cv}$ data  i.e. ` `$f_{cv}$ ```= decision_function(```$X_{cv}$```)```  <font color='red'>you need to implement this decision_function()</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fCgMNEvI4Zxf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ANUNIqCe4Zxn"
   },
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=5000, n_features=5, n_redundant=2,\n",
    "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting Data for tain_test, and CV\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.4, random_state=42)#splitting in the ratio of 60:40\n",
    "X_CV, X_test, y_CV, y_test = train_test_split( X_test, y_test, test_size=0.5, random_state=42)#splitting in the ration 60,20,20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHie1zqH4Zxt"
   },
   "source": [
    "### Pseudo code\n",
    "\n",
    "clf = SVC(gamma=0.001, C=100.)<br>\n",
    "clf.fit(Xtrain, ytrain)\n",
    "\n",
    "<font color='green'>def</font> <font color='blue'>decision_function</font>(Xcv, ...): #use appropriate parameters <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color='green'>for</font> a data point $x_q$ <font color='green'>in</font> Xcv: <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color='grey'>#write code to implement $(\\sum_{i=1}^{\\text{all the support vectors}}(y_{i}\\alpha_{i}K(x_{i},x_{q})) + intercept)$, here the values $y_i$, $\\alpha_{i}$, and $intercept$ can be obtained from the trained model</font><br>\n",
    "   <font color='green'>return</font> <font color='grey'><i># the decision_function output for all the data points in the Xcv</i></font>\n",
    "    \n",
    "fcv = decision_function(Xcv, ...)  <i># based on your requirement you can pass any other parameters </i>\n",
    "\n",
    "<b>Note</b>: Make sure the values you get as fcv, should be equal to outputs of clf.decision_function(Xcv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "h43kDT3M41u5"
   },
   "outputs": [],
   "source": [
    "# you can write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100, gamma=0.001)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf=SVC(gamma=0.001,C=100,kernel='rbf')\n",
    "clf.fit(X_train,y_train)#Trainig the SVC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k(xi,xq,gamma):#kernel function that \n",
    "    temp=0\n",
    "    from math import exp\n",
    "    x=xq-xi\n",
    "    x=x**2\n",
    "    x=x*gamma\n",
    "    x=sum(x)\n",
    "    return exp(-x)#return a value\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_function(X_CV,model,SV,gamma):\n",
    "    fcr=[]\n",
    "    for xq in X_CV:# for every point in X_CV\n",
    "        temp=0#storing in a variable to sum up with all the SV vectors\n",
    "        for a,xi in zip(model.dual_coef_.flatten(),SV):\n",
    "            temp=temp+(a*(k(xi,xq,gamma)))\n",
    "        fcr.append(temp+model.intercept_[0])\n",
    "    return fcr\n",
    "#SV----> support vectors of size 5 dimensions\n",
    "#a----> yi*alphai we can get this value from model.dual_coef_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SV=clf.support_vectors_\n",
    "temp2=decision_function(X_CV,clf,SV,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=clf.decision_function(X_CV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of points equal in both the sets = 979\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of points equal in both the sets = {(temp==temp2).sum()}')\n",
    "fcv=temp2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0bKCboN4Zxu"
   },
   "source": [
    "<h2><font color='Geen'> 8F: Implementing Platt Scaling to find P(Y==1|X)</font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMn7OEN94Zxw"
   },
   "source": [
    "Check this <a href='https://drive.google.com/open?id=133odBinMOIVb_rh_GQxxsyMRyW-Zts7a'>PDF</a>\n",
    "<img src='https://i.imgur.com/CAMnVnh.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0n5EFkx4Zxz"
   },
   "source": [
    "## TASK F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0HOqVJq4Zx1"
   },
   "source": [
    "\n",
    "> 4. Apply SGD algorithm with ($f_{cv}$, $y_{cv}$) and find the weight $W$ intercept $b$ ```Note: here our data is of one dimensional so we will have a one dimensional weight vector i.e W.shape (1,)``` \n",
    "\n",
    "> Note1: Don't forget to change the values of $y_{cv}$ as mentioned in the above image. you will calculate y+, y- based on data points in train data\n",
    "\n",
    "> Note2: the Sklearn's SGD algorithm doesn't support the real valued outputs, you need to use the code that was done in the `'Logistic Regression with SGD and L2'` Assignment after modifying loss function, and use same parameters that used in that assignment.\n",
    "<img src='https://i.imgur.com/zKYE9Oc.png'>\n",
    "if Y[i] is 1, it will be replaced with y+ value else it will replaced with y- value\n",
    "\n",
    "> 5. For a given data point from $X_{test}$, $P(Y=1|X) = \\frac{1}{1+exp(-(W*f_{test}+ b))}$ where ` `$f_{test}$ ```= decision_function(```$X_{test}$```)```, W and b will be learned as metioned in the above step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTY7z2bd4Zx2"
   },
   "source": [
    "__Note: in the above algorithm, the steps 2, 4 might need hyper parameter tuning, To reduce the complexity of the assignment we are excluding the hyerparameter tuning part, but intrested students can try that__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CM3odN1Z4Zx3"
   },
   "source": [
    "\n",
    "If any one wants to try other calibration algorithm istonic regression also please check these tutorials\n",
    "\n",
    "1. http://fa.bianp.net/blog/tag/scikit-learn.html#fn:1\n",
    "\n",
    "2. https://drive.google.com/open?id=1MzmA7QaP58RDzocB0RBmRiWfl7Co_VJ7\n",
    "\n",
    "3. https://drive.google.com/open?id=133odBinMOIVb_rh_GQxxsyMRyW-Zts7a\n",
    "\n",
    "4. https://stat.fandom.com/wiki/Isotonic_regression#Pool_Adjacent_Violators_Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_plus=(y_train[y_train==1].sum()+1)/(y_train[y_train==1].sum()+2)\n",
    "Y_minus=1/(y_train[y_train==0].sum()+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = lambda x,y_plus,Y_minus: Y_plus if x==1 else Y_minus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_cv_Y=np.array([t(x,Y_plus,Y_minus) for x in y_CV])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_cv_X=decision_function(X_CV,clf,SV,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=0\n",
    "w=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(b)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(w,b, X):\n",
    "    N = len(X)\n",
    "    predict = []\n",
    "    for i in range(N):\n",
    "        z=np.dot(w,X[i])+b\n",
    "        predict.append(sigmoid(z))\n",
    "    return np.array(predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(row_vector):\n",
    "    ''' In this function, we will initialize our weights and bias'''\n",
    "    #initialize the weights as 1d array consisting of all zeros similar to the dimensions of row_vector\n",
    "    #you use zeros_li`ke function to initialize zero, check this link https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros_like.html\n",
    "    #initialize bias to zero\n",
    "    w=np.zeros_like(row_vector)\n",
    "    b=0\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#make sure that the sigmoid function returns a scalar value, you can use dot function operation\n",
    "def gradient_dw(x,y,w,b,alpha,N,eta):\n",
    "    '''In this function, we will compute the gardient w.r.to w '''\n",
    "    #temp1=np.sum(np.dot(w.T,x),b)\n",
    "    temp=np.dot(w.T,x) + b\n",
    "    si=sigmoid(temp)\n",
    "    temp2=np.subtract(y,si)\n",
    "    temp3=np.multiply(x,temp2)\n",
    "    learning_rate=eta\n",
    "    temp4=learning_rate/N  * w\n",
    "    dw=np.subtract(temp3,temp4)    \n",
    "    return dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "def sigmoid(z):\n",
    "    ''' In this function, we will return sigmoid of z'''\n",
    "    # compute sigmoid(z) and return\n",
    "    return 1/(1+exp(-z))\n",
    "#sb should be a scalar value\n",
    "def gradient_db(x,y,w,b):\n",
    "     '''In this function, we will compute gradient w.r.to b '''\n",
    "     temp=np.dot(w.T,x) + b\n",
    "     si=sigmoid(temp)\n",
    "     db=np.subtract(y,si) \n",
    "     return db\n",
    "import numpy as np\n",
    "def logloss(y_true,y_pred):\n",
    "    # you have been given two arrays y_true and y_pred and you have to calculate the logloss\n",
    "    #while dealing with numpy arrays you can use vectorized operations for quicker calculations as compared to using loops\n",
    "    #https://www.pythonlikeyoumeanit.com/Module3_IntroducingNumpy/VectorizedOperations.html\n",
    "    #https://www.geeksforgeeks.org/vectorized-operations-in-numpy/\n",
    "    #write your code here\n",
    "    y_true2=np.ones_like(y_true)\n",
    "    y_pred2=np.ones_like(y_pred)\n",
    "    y_true3=np.subtract(y_true2,y_true)\n",
    "    y_pred3=np.subtract(y_pred2,y_pred)\n",
    "    loss= np.sum(np.add(np.multiply(y_true, np.log10(y_pred)), np.multiply(y_true3 , np.log10(y_pred3))))\n",
    "    return -1*loss/len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train,y_train,epochs,alpha,eta0):\n",
    "    ''' In this function, we will implement logistic regression'''    \n",
    "    train_loss = []\n",
    "    w,b = initialize_weights(X_train[0])# Initialize the weights\n",
    "    tempw,tempb=0,0\n",
    "    #write your code to perform SGD\n",
    "    N=len(f_cv_X)\n",
    "    for i in range(epochs):\n",
    "        for x,y in zip(X_train,y_train):# for every data point(X_train,y_train)\n",
    "            dw=gradient_dw(x,y,w,b,alpha,N,eta0)          #compute gradient w.r.to w (call the gradient_dw() function)\n",
    "            db=gradient_db(x,y,w,b)      #compute gradient w.r.to b (call the gradient_db() function)\n",
    "            w=w+(alpha*dw)  #update w, b\n",
    "            b=b+(alpha*db)\n",
    "        predect=pred(w,b,X_train)\n",
    "        losst=logloss(y_train,predect)\n",
    "        train_loss.append(losst)    \n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss=train(f_cv_X,f_cv_Y,20,0.01,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X=[i for i in range(20)]\n",
    "Y=train_loss\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(X,Y)\n",
    "plt.xlabel('epcho')\n",
    "plt.legend(['Train_loss','Test_loss'])\n",
    "plt.title('Epcoh vs losses')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftest=decision_function(X_test,clf,SV,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "def exp_fun(w,b,ftes):\n",
    "    temp=ftes  + b\n",
    "    return 1/(exp(-temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=[]\n",
    "for i in ftest:\n",
    "    temp.append(exp_fun(w,b,i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1990384349125596, 0.13042038667622374, 0.4548684318170676, 0.11002794978555795, 0.022129427427553586, 0.04689348272639252, 0.08789884098149284, 18.576051063634427, 0.03406817979518741, 3.5341694893532902, 0.11885283286147293, 2.7118709911361925, 0.612858925288496, 0.13255236828954559, 0.4736916150793697, 28.19836823904677, 0.03771499011667371, 0.051060772424962826, 0.29212513701967496, 0.05696777146597392, 6.892176620433089, 0.1218779788855356, 5.188133390254035, 0.2938303205563454, 0.1148574019159291, 2.0408286084771925, 5.653544431621272, 0.4119203343204511, 0.060557436281172924, 2.6226400676346726, 0.11316094242444578, 10.013053940665682, 0.172796554667986, 0.030208614196050393, 0.05200957785305591, 0.25603054949564397, 0.28408325796502804, 0.09715455445164181, 7.302077405625844, 0.03418550011148117, 0.014629537901782603, 1.458793145778746, 0.10317810228291979, 0.04771389003594695, 0.048337353496522004, 0.16768508116257577, 0.05391680737421425, 0.08238662952488662, 4.2500523420658824, 0.16253228960732133, 0.060306513985778076, 0.09855959305726927, 0.08740428456946633, 1.486812295109756, 4.908632760046323, 8.372042561855183, 0.02897683091765853, 11.91382393619438, 2.2023823066704833, 0.045491576997814975, 0.1010893696844716, 6.878285863779243, 0.11198181978330152, 0.0645123269948899, 0.15300878675141286, 0.43575230986362035, 0.03870979327034872, 0.03175311758854435, 0.41256743699311177, 0.037854602593494824, 0.0788547974994019, 0.7640808135400393, 4.7415126488558945, 0.16423196813854657, 0.04802289779160676, 0.1982325582002952, 0.4259499471349618, 0.21717097286833845, 0.03186058879682192, 0.05647152328000247, 1.7482968459398942, 0.027487320937159714, 0.16035473178523155, 2.2434852577591005, 10.892603735351308, 1.936931114827374, 0.211664518282626, 3.931655322553758, 0.15134608957298254, 1.4512817589325173, 2.267610253702226, 0.036166217874574226, 0.20399073125757908, 0.031228849331205177, 6.207034490628861, 0.6550609260559211, 0.0622020458909968, 0.2312269680516894, 3.4331179729312864, 2.6395703233486048, 4.898554893586495, 6.942252501463559, 4.41707566240242, 2.2894458366662533, 3.322060826785597, 0.088105920771933, 6.583616536330761, 0.27178457882512863, 0.3412970802372931, 0.06207928212457218, 0.04866078752408683, 0.026562394686151774, 0.06108868837067765, 2.1973994596557773, 0.011566140497069704, 0.05545198033160963, 0.09430691313610974, 0.08568006756483312, 5.123386801884291, 9.031304136721845, 4.688599647381936, 0.4619662511253166, 0.042563099820240397, 0.13297942600741722, 2.351036061836681, 0.015795386970075067, 0.12030514342553833, 5.566538795242764, 3.681242813053486, 1.4773234188402695, 0.17728917118059273, 0.05742594603494959, 0.016791660210992904, 0.016739272274886845, 0.05785406208819822, 1.7804790272277713, 0.048865176435323045, 0.2032599370404507, 0.0902566229332576, 0.023942446304441815, 0.07243076893779221, 9.812273485626074, 0.15665615713324504, 0.04858831028630013, 0.06464872498594147, 0.010715145793508964, 0.04747031647848512, 0.0445266147857559, 3.2611327188006247, 0.0903359099194136, 5.434781541815629, 8.38706838358231, 0.9757680608124204, 0.2557533448422831, 0.04226508474331299, 0.05963047863595911, 0.04697523225436661, 0.05622234702413362, 0.04903001690719441, 3.8470469982813222, 0.048416643533521604, 0.9731374937597679, 5.060154245383749, 0.008062183930442796, 0.16533425417653091, 0.04575092982746565, 0.04953702714632729, 0.24863407990315856, 1.888116624929422, 0.11977417591475954, 0.4422655341105047, 10.358754640063632, 5.25060286644809, 0.022020145346042012, 0.09504355432819438, 0.10767924719152874, 0.06623696434606338, 0.08615844431563138, 0.2761745595095393, 0.2017007667764851, 0.010324992778535888, 5.593969912198338, 4.156065099587662, 0.18690136498413326, 7.167930299427721, 0.22763278036723222, 5.15802379497378, 0.013053347840325776, 11.438435311149036, 0.08951609363743215, 0.07907971286732075, 3.2018062288524773, 1.348231105232967, 0.034377357575128296, 6.349025926328691, 0.05752123579349908, 0.09219596434710872, 0.09588013755771882, 0.072189019788588, 0.3448953875360029, 0.11646990764314046, 0.09320750104284554, 6.351400195674592, 1.1933509373233269, 0.21013929194255937, 0.01585078071047978, 5.087377062932173, 4.434144608687535, 0.09608620246859123, 4.6041306519202765, 0.23571589769809437, 0.0649381789343674, 0.03694427698955341, 19.417723771749216, 0.0630347092780666, 0.07634245944534294, 0.5953620348350686, 3.934208277489238, 0.09139430267886296, 0.1567709871734467, 0.05939625267069701, 0.07921061680548327, 0.10282636741228332, 0.09429936785727738, 6.307996790688975, 0.4449784871602628, 0.06570525357599859, 0.03389504059698026, 1.5344716603801176, 0.017888243344007213, 0.03227381189432286, 0.05626562157448676, 0.2872875446347851, 0.1184379573911561, 0.027252955184095186, 0.10962468211866871, 0.021008227339371053, 0.1761412927115139, 1.4601117693210242, 0.06578681776549115, 4.427363000504013, 0.14275030064942085, 0.09515460099901064, 4.668384362247437, 0.23132595902378045, 4.996557705450738, 0.37199163063599094, 0.31989618402301156, 0.17295768520948282, 0.1664672480956766, 1.5777198292438686, 0.42762453981256954, 9.346472795812648, 1.3844197193736614, 0.09197900639234552, 1.0529152291855155, 3.6415621832232876, 1.1941684072694385, 0.7023958466618504, 4.148070237288259, 0.3971584642305965, 0.15079756804051145, 0.020899508421094504, 0.11516574966405042, 0.11634281534272517, 0.17733004417234569, 0.4747480021537325, 0.8832684325282871, 0.1803230861044579, 1.083789966556583, 0.0262121482626064, 0.2258741244440689, 0.3414360830461436, 0.09011911605959486, 0.102031506438154, 9.083379289879424, 9.623881270921794, 1.9343659809787992, 0.29185285664039784, 9.650727902725956, 0.03498780486661349, 2.175492654603881, 4.537756810616766, 0.1638474357682973, 0.07667001403417929, 1.839376623448945, 0.07502140390143362, 0.1976194082631581, 0.03429314996891683, 0.41796794849754554, 0.4899905813906166, 2.266554992004731, 4.3992485677307345, 0.05814025058267568, 0.0809786882194019, 0.39577265028216974, 6.001631402668052, 0.6322193878229195, 1.394519669343754, 7.78207703629869, 3.3382614806377284, 0.7262250188025823, 5.6327485426383115, 0.22391535010111746, 5.660521679111207, 0.04725683117788995, 2.962873906800802, 0.10268210430190798, 0.8612129747478611, 0.08698687912201503, 0.1147780905298551, 0.04835109012626189, 0.12763894948099253, 0.3539040479870342, 0.06561882787676826, 0.005017430961301858, 2.949216300352139, 0.025761204444974282, 0.2830915198105584, 0.05914662787394915, 0.0551710124839858, 5.759884770253636, 1.497792994698459, 0.0930841791615649, 0.07245695519391454, 0.08045323422074717, 0.04964445953919595, 0.15787955218303867, 0.255671086129585, 0.7885744770230934, 0.011152618868138552, 0.13489758790980363, 8.634452055670554, 0.6771552919299518, 0.3170330897772616, 1.0837285567474098, 0.026608142699155013, 8.496150363838884, 3.0521989379066623, 0.2889050724666469, 0.1293973051275788, 0.09850457494603478, 6.345582229311573, 0.061713800341293146, 0.31574625708230986, 0.05614869475061503, 0.023342763894045822, 3.515543615552259, 0.15141846276754656, 0.16035681468400403, 0.8493798532521031, 0.5289787394987598, 0.03915718109492194, 0.037564783823028046, 0.157493645981296, 0.5431882412744924, 0.3863290256699462, 1.3291745167962756, 3.5861681700705414, 0.35736460795492037, 1.9944738616399642, 0.07274256114412266, 2.5676866547630337, 0.06203310865076874, 0.05679198650973252, 1.5442913867586403, 0.2704792054330784, 0.15924741008487964, 0.08728207748593761, 2.7000264784864947, 0.11467601341809264, 0.2543848033385362, 5.134599435827591, 1.6371103100175568, 3.7716439288045716, 0.06524273838399276, 0.1563303233553265, 7.758562693046634, 6.531721662368638, 4.298200523759193, 5.7961931080979525, 4.856735870004874, 0.04766577130330387, 0.10799373196967786, 0.7829316011034736, 0.86237879290336, 0.04605823626528087, 0.1086315541352202, 0.05328176265716017, 0.15036476825300665, 0.7033613121771664, 0.0208054097811587, 0.04560497101311653, 0.03289722246159091, 0.21791787712055738, 0.3668824233268173, 1.7383070331069979, 0.045426984672911404, 0.05412662540186483, 0.02922751455616745, 0.7226280720634205, 0.6198828289838875, 0.11722744247822459, 9.08310847529101, 0.05087959263138582, 0.05782526131944655, 0.10913642119340393, 0.04823555873884944, 0.6189195301419951, 0.13006747826541254, 3.523018181118128, 0.09899621434290218, 0.13730645785243997, 0.7193005034430731, 0.03241059440066973, 3.4976001861982775, 0.09707007532379391, 0.21176948957468583, 0.7881066714643303, 0.20497779637996638, 0.1617600555432714, 0.04712187124877776, 0.04746553154061125, 3.634700325590975, 0.07866931933529031, 4.696447157580087, 0.046450873859351496, 2.3970181143293656, 0.05279310145250239, 0.08209352243356462, 0.13523514162132225, 0.1875970905484572, 3.770457228665049, 0.19343574347067347, 0.0784627627865068, 7.334592179137668, 0.07059124662637101, 0.03292749529835976, 0.11770318496334048, 0.15079808814440707, 0.12708636966039813, 0.03840093340237266, 6.435282506101102, 0.19192729024059318, 0.03276531435018142, 0.046369427563768856, 0.06903144950095244, 0.030978978540217004, 0.46727074954355863, 0.015881589535217826, 5.741648716880407, 0.144093123163907, 0.5295329177929544, 0.18870777616533152, 0.5761288480160721, 0.378152952642545, 2.3374526361735968, 0.6208542899280339, 0.025843032650003858, 0.09069151343006575, 6.880444903177291, 0.19872668617846703, 2.620229166701349, 0.023738421011331383, 3.7274709228529237, 0.027705857819896804, 0.11661185698910209, 0.05851586286832724, 0.02851324545473148, 0.05665597767893867, 0.0643438719286669, 5.868564586153901, 5.2146935147821445, 0.3313865750678202, 10.564790840895446, 4.744053062771775, 0.023367755652036287, 7.318299146675176, 2.5536056144763215, 0.39369588371998593, 8.812968378836063, 1.1554349436547013, 0.8863920308030683, 0.49768852505679556, 58.989193285224744, 0.04488444751985015, 3.443501992179223, 0.2713217431767199, 6.511066876187797, 3.7079358808243, 0.081915089952, 1.089954556168743, 0.30004521468905304, 0.06383287744725831, 0.10854600132491395, 0.06696463385543869, 1.3890912820673533, 0.03772573407755818, 6.828031122525413, 0.2265579265764297, 0.010295652244576577, 0.3724120306531106, 5.950883737452462, 4.461937539084825, 0.29811178985547426, 0.5162504299490922, 0.16702546690014208, 0.5265091672680223, 0.07738536795746086, 3.5901365479691907, 0.999799571754159, 14.39063643619138, 0.1614187727216364, 5.789423084730187, 5.564883368247802, 0.07815329907316948, 2.557670560640231, 0.035564604233573605, 0.03235444114206368, 4.9959187426448075, 0.03567548253878319, 2.321913148981933, 0.10702405315138004, 1.4201797987250284, 5.830462694288921, 0.03247162279152282, 5.57375508140797, 0.019595849120630268, 0.0683593280634158, 0.5522549651861299, 0.054981222939947574, 0.1784402690058183, 5.05540640037539, 6.449917313815492, 0.17834410968457823, 0.00807708722290036, 0.7313544776514775, 1.6433835978840894, 0.05621884711290424, 0.04613633559827997, 0.07192044175098145, 0.14210733839538403, 0.019809613331320402, 4.935559085403752, 18.673313213750546, 8.0316719723952, 7.035889399486422, 0.41152978014155256, 0.06380113265547076, 0.0979956401916792, 2.826095761244704, 1.0260723100645315, 0.997897300051498, 0.30454819411879225, 3.4148557658187952, 0.06489222365676546, 0.7377488565190753, 0.039019627834712606, 0.08667087579517448, 0.034509460194183536, 0.4995820830539336, 1.1312829917820828, 6.575011456064863, 0.1535052261994721, 0.2731774102345919, 3.2112286937149888, 14.625206831812578, 0.11770343043556916, 0.05196848311271436, 0.09291575141814247, 0.20580579994831258, 0.26561126394227325, 0.09010038887272583, 0.04954720418607011, 0.11605534890068522, 0.05501107870219379, 0.05736762742696685, 0.027606970941538533, 0.046723683552731266, 0.0867907268741896, 0.047092542970521484, 0.8770977328325411, 0.013124927212092914, 8.446891761910148, 0.11656578706406708, 4.2681592619792035, 0.4018733405735616, 0.06832358115311192, 1.7360610442390239, 0.06053829168825724, 3.875532605839003, 5.030055013298509, 8.50209912648541, 0.0981197170984329, 0.016411792030535204, 0.49120615358737246, 0.2937309792464206, 0.1323296752974404, 0.575670936358185, 0.25318133788406916, 0.014530462495202496, 0.03415647572005542, 0.034137601520364685, 0.09850859324279011, 7.239792457587727, 0.142790851330063, 0.547374998338233, 0.08004143814412062, 0.07062298407195548, 6.527015267743565, 5.788131016554897, 6.1708870446115975, 0.6985702020992868, 0.007426972751448979, 0.07078214988525282, 5.416115229233355, 0.08684476532646847, 0.09322492159719088, 0.09568433425503686, 6.095945631193126, 0.03202965715982226, 0.05237460041574893, 2.7470515903445754, 0.186687292138801, 0.21185894646335185, 0.038927384549545835, 0.05354199945624711, 0.16164621845356983, 0.07076125983881648, 6.574272568044507, 0.019118857069590312, 0.1305622941938147, 0.07792835089678436, 0.7242478666413343, 3.2326116445083497, 0.03290391015554786, 0.4952947467911145, 0.12492680645711883, 4.242694474768931, 0.03348237712773211, 2.1494381801231928, 0.11594864650480392, 0.0678776088412228, 0.06526103092171137, 0.2828245149907476, 0.01624939106294507, 0.04742740637566373, 0.7608311038568986, 0.08966193616927343, 0.07202234164439826, 0.08498833881378824, 0.058113261500442576, 0.11190573651444888, 0.20653436339758724, 6.615414482803819, 0.23599550371924796, 1.1122339641584973, 0.05806132516314035, 0.08160840854346599, 0.07419216377156099, 4.882105102591767, 0.10447113358772377, 0.07640318168602699, 0.1430311027032067, 0.016858275730395923, 0.10929002055797468, 2.889200606071526, 0.10324088805868048, 7.943301537374262, 4.690555687814552, 0.19254408926196928, 0.0879379047885572, 0.11080506677243192, 2.1391086045632726, 2.8975431774275355, 0.0907372184698532, 0.048045094736262194, 0.11690800017093365, 0.06969244606021117, 0.12202737942518538, 0.5093551445485046, 2.135188781008948, 5.820273241818069, 0.041963332521066835, 0.03867657888761077, 0.5868352194556509, 2.75644967992792, 0.017491360751731366, 0.4311635299977446, 5.12513178402141, 0.05441944381628596, 0.009812909576716712, 0.054479542026530055, 0.18652072302262762, 0.05101742049222104, 4.612190137047505, 0.5722636917752163, 1.42576059132097, 0.09259405714503594, 0.5678089074328758, 9.950150686617047, 0.8536802770558989, 0.7151867208778695, 0.5030157109930028, 0.06858520546617353, 0.08436054948832823, 9.542122785163382, 0.22586918604086773, 0.050133317326475, 0.02915049500759649, 3.0373450103376007, 0.05551980354225402, 0.018590381463047114, 0.08597756579503406, 0.10340858923033976, 1.541527063901695, 0.07943705288848055, 0.07170612727058899, 4.96009372255247, 7.416091850963037, 0.03352574152325435, 0.2380393770492294, 0.43504624072187636, 0.04967852534601726, 1.0130108592701306, 0.21445168472450965, 0.03583874006587704, 0.41219188972257415, 0.5858378167516538, 0.4652386209799323, 0.13259113462807437, 0.38343648448693307, 0.06092767610733608, 3.9410441035204355, 0.15547709870118662, 0.09745340740130477, 0.08256018431307127, 0.11042412827194313, 0.19523889008989936, 1.0181487834514866, 1.406319350960598, 0.11884622220339112, 3.7016644647483283, 0.14244360919868482, 2.98908525148879, 0.15056010965080727, 0.030740749848866676, 0.17212693240795476, 0.03189887506992662, 0.8529191595330834, 5.762553011785445, 0.03254306391567291, 0.8690849522186105, 0.3329832993256995, 0.0522551916631854, 0.7117072731554286, 0.19456216020327227, 0.17574488272176708, 0.1293730454242682, 1.9719122010957426, 5.391232465318214, 1.1318881030530301, 0.5126330495571179, 5.619393941853867, 0.40456159765113076, 0.09801130031071795, 0.19443880688516182, 8.18062675569478, 8.214261452108222, 4.369815254756828, 2.207219651475896, 0.321930383444948, 0.8905836966931755, 0.07660718031085927, 3.0109793985831734, 0.1865750608500367, 10.089300686868837, 6.227781282078232, 0.3735224036857591, 0.19991648230826128, 0.010364870545604674, 0.23435954990410018, 3.4142122956403735, 4.052383145851897, 0.23192660996143588, 0.16697999506617928, 0.07928824098172312, 0.22659490054577996, 0.060521302023278406, 0.12238535406828188, 8.225588667119359, 4.257706338804405, 0.021409472482364834, 5.162507701898714, 4.949818542754839, 0.0759522084825641, 0.47065904070247494, 0.20693138781883944, 0.06084759890331014, 0.6447326974512375, 3.997785748605986, 7.129450262014452, 6.6837382825114116, 0.04174266284177258, 0.05116115932166187, 0.0689357831920709, 17.56888970835694, 0.2039463882412837, 0.26236393918292683, 5.316240445068045, 0.12633988799207027, 0.20046847802510623, 0.07827002410650678, 0.2524367003130988, 0.0400511715992636, 3.7432342206033336, 0.27730822969665997, 0.05063004846330218, 0.13775312950825813, 0.032406021713872225, 8.276175931799942, 5.913639110403415, 0.04372279465371328, 0.8761520380720909, 3.594209957497673, 9.235370387075754, 0.0854994658115079, 1.1608307252427916, 0.08673793437221763, 0.052354287245743876, 5.835163010374287, 9.33443650891878, 0.15962592302015183, 0.4664431705331748, 0.6539906137276831, 0.48460963459505146, 2.9079060181776963, 0.09633909771710139, 0.3352873468053147, 0.008365120905182313, 0.012924945302854712, 0.1969976764699557, 2.804085557386776, 3.0415178139458376, 0.187720286540408, 0.040661728417409566, 0.09053135119587365, 0.022355715624519473, 0.02421067785792369, 0.1463356682854465, 0.03696206743062447, 0.21946988063065037, 0.02280179963645574, 1.954384971551982, 0.29711621121769893, 0.08046723071691952, 0.1632875622559195, 1.7819742164650938, 8.145362840433643, 0.2616670045556029, 0.07863611948903079, 4.855791212431799, 0.18618203890360804, 0.04064853783366766, 6.746303069358536, 1.3493821555535144, 0.01954066907893551, 1.5478265464918157, 0.07351506620352706, 0.12945968966389135, 0.08024053606123659, 4.350404666021362, 0.5024214191092242, 4.425925743100413, 3.77959942920096, 0.11139505007567724, 0.0729400523194947, 0.2372393425042456, 0.05135401861776242, 0.034241852779541934, 0.1884207101918736, 0.2221621094245538, 0.03292619614952058, 0.09734791407821064, 0.23336632887429637, 0.1104314452587805, 0.4089815222647348, 4.52697675431054, 0.9788991067135387, 0.14779045821718928, 6.32813781487662, 0.05017759507576518, 0.09990642743222962, 3.2683791283328167, 0.06909175251107366, 0.6397716936996393, 0.020925588068341466, 0.058284941286327996, 1.4752016371712242, 0.051604225100332725, 0.028255579589957388, 0.06642516963160164, 0.02349431907474709, 0.01459830113644496, 1.0962439655014908, 0.1609458611878162, 0.5661883407947339, 5.606245577577955, 0.26122718095260045, 6.110766834935329, 0.03067705371392887, 0.08809152812483087, 1.98373823738326, 0.11216620728472798, 0.18943372852854373, 0.24345856136477936, 0.1506906454215694, 0.04689517798973253, 1.8327417808062771, 0.02763529515951555, 0.5146889113935058, 0.12703444256419666, 5.44776598498044, 0.05464438229687879, 0.8766906226203449, 0.6865528200543768, 5.751323636583924, 0.031368658626617925, 0.13498305264163246, 4.349815877385736, 0.36553935934097326, 0.3350012037714977, 11.056558357454671, 0.6499637366669242, 5.879295770033022, 0.23546882899358473, 0.24072030987759832, 2.927034143789611, 0.09545051571089098, 0.7564216954241324, 0.39781851702407006, 0.067899800756327, 0.1274502398744705, 0.2078872069804697, 0.048670905738357084, 0.17158015725345077, 5.445008563626455, 0.10562380035733726, 0.24562566045269868, 0.177211912775613, 0.05327383150857194, 0.45965658628945694, 1.212638433844639, 0.09767804536091579, 0.03025506523374657, 0.11450955474657619, 0.04993628171446166, 0.02035253936555691, 0.5038777113979475, 0.03549153464379482, 0.08686267046273999, 0.14426466027532406, 0.01894473357977008, 0.029454417317893822, 0.3252882885648389, 0.10880670660163282, 0.050072432810283854, 6.447841925212524, 0.04919532732701011, 34.421144198468475, 3.9668710423442968, 0.2007857440199108, 0.05133885096876533, 0.4854890697393565, 0.47207125261239996, 0.1720128363010237, 0.015782610201209184, 0.022903966319322004, 0.14135976060254257, 0.16808408808535558, 0.017985168338778414, 0.04762942914182645, 0.13398650961127304, 0.8832176607120565, 0.05868054962844335, 0.061135176776227186, 0.01693674314723879, 0.0644852070928685, 0.5399799901233356, 0.7217020032265304, 0.012000023906225512, 1.582966717716, 3.647624626056127, 0.05080130161083468, 0.21118233689831573, 0.053640892729491615, 0.11427386612372831, 0.399487606335277, 3.8477060183198404, 0.7385606551456064]\n"
     ]
    }
   ],
   "source": [
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "8E&F_LR_SVM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
